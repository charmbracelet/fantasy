---
version: 2
interactions:
- id: 0
  request:
    proto: HTTP/1.1
    proto_major: 1
    proto_minor: 1
    content_length: 436
    host: ""
    body: '{"messages":[{"content":"You are a helpful assistant","role":"system"},{"content":"What''s the weather in Florence, Italy?","role":"user"}],"model":"grok-3-mini","tool_choice":"auto","tools":[{"function":{"name":"weather","strict":false,"description":"Get weather information for a location","parameters":{"properties":{"location":{"description":"the city","type":"string"}},"required":["location"],"type":"object"}},"type":"function"}]}'
    form:
      api-version:
      - 2025-01-01-preview
    headers:
      Accept:
      - application/json
      Content-Type:
      - application/json
      User-Agent:
      - OpenAI/Go 2.3.0
    url: https://fantasy-playground-resource.services.ai.azure.com/openai/deployments/grok-3-mini/chat/completions?api-version=2025-01-01-preview
    method: POST
  response:
    proto: HTTP/2.0
    proto_major: 2
    proto_minor: 0
    content_length: -1
    uncompressed: true
    body: |
      {"choices":[{"content_filter_results":{},"finish_reason":"tool_calls","index":0,"logprobs":null,"message":{"content":"","reasoning_content":"First, the user is asking about the weather in Florence, Italy. I have an available function called \"weather\" that can get weather information for a location.\n\nThe function description is: \"Get weather information for a location\". The parameters require a \"location\" which is a string describing the city.\n\nIn this query, the user has provided \"Florence, Italy\", which clearly specifies the city and country. This should be sufficient as the location parameter.\n\nAll required parameters are provided and inferable. The location is given as \"Florence, Italy\", so I can use that directly.\n\nI should call the function if it can advance the user's request. Here, calling the \"weather\" function will directly provide the weather information, fulfilling the user's query.\n\nThe function call format must be in JSON within \u003cfunction_call\u003e tags: \u003cfunction_call\u003e{\"action\": \"weather\", \"action_input\": {\"location\": \"Florence, Italy\"}}\u003c/function_call\u003e\n\nI need to ensure my response is not verbose. So, I should only include the function call if that's the next step, without additional text unless necessary.\n\nSince this is a direct match, I don't need to ask for clarification or add extra explanations.\n\nFinally, after calling the function, in a multi-turn conversation, I might need to handle the response, but for now, the logical next step is to call this function.\n\nResponse structure: I should output only the function call in the specified format, as per the instructions.","refusal":null,"role":"assistant","tool_calls":[{"function":{"arguments":"{\"location\":\"Florence, Italy\"}","call_id":null,"name":"weather"},"id":"call_47156543","type":"function"}]},"stop_reason":null}],"created":1759350320,"id":"e6688144-8493-4d1a-b0b8-0a06d0515dd7_sidecar","model":"grok-3-mini-high","object":"chat.completion","prompt_filter_results":[{"prompt_index":0,"content_filter_results":{"hate":{"filtered":false,"severity":"safe"},"jailbreak":{"filtered":false,"detected":false},"self_harm":{"filtered":false,"severity":"safe"},"sexual":{"filtered":false,"severity":"safe"},"violence":{"filtered":false,"severity":"safe"}}}],"usage":{"audio_prompt_tokens":0,"completion_tokens":27,"completion_tokens_details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":294,"rejected_prediction_tokens":0},"num_sources_used":0,"prompt_tokens":288,"prompt_tokens_details":{"audio_tokens":0,"cached_tokens":0,"image_tokens":0,"text_tokens":288},"total_tokens":609}}
    headers:
      Content-Type:
      - application/json
    status: 200 OK
    code: 200
    duration: 4.146192208s
- id: 1
  request:
    proto: HTTP/1.1
    proto_major: 1
    proto_minor: 1
    content_length: 656
    host: ""
    body: '{"messages":[{"content":"You are a helpful assistant","role":"system"},{"content":"What''s the weather in Florence, Italy?","role":"user"},{"tool_calls":[{"id":"call_47156543","function":{"arguments":"{\"location\":\"Florence, Italy\"}","name":"weather"},"type":"function"}],"role":"assistant"},{"content":"40 C","tool_call_id":"call_47156543","role":"tool"}],"model":"grok-3-mini","tool_choice":"auto","tools":[{"function":{"name":"weather","strict":false,"description":"Get weather information for a location","parameters":{"properties":{"location":{"description":"the city","type":"string"}},"required":["location"],"type":"object"}},"type":"function"}]}'
    form:
      api-version:
      - 2025-01-01-preview
    headers:
      Accept:
      - application/json
      Content-Type:
      - application/json
      User-Agent:
      - OpenAI/Go 2.3.0
    url: https://fantasy-playground-resource.services.ai.azure.com/openai/deployments/grok-3-mini/chat/completions?api-version=2025-01-01-preview
    method: POST
  response:
    proto: HTTP/2.0
    proto_major: 2
    proto_minor: 0
    content_length: -1
    uncompressed: true
    body: |
      {"choices":[{"content_filter_results":{"hate":{"filtered":false,"severity":"safe"},"protected_material_code":{"filtered":false,"detected":false},"protected_material_text":{"filtered":false,"detected":false},"self_harm":{"filtered":false,"severity":"safe"},"sexual":{"filtered":false,"severity":"safe"},"violence":{"filtered":false,"severity":"safe"}},"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"The weather in Florence, Italy is 40°C.","reasoning_content":"First, the user asked about the weather in Florence, Italy. I have an available function called \"weather\" that can get weather information for a location. The required parameter is \"location\", which is a string.\n\nIn the user's query, the location is clearly provided as \"Florence, Italy\". So, all required parameters are present and inferable. Therefore, I should call the weather function.\n\nI need to use the exact format for the function call: within \u003cfunction_call\u003e\u003c/function_call\u003e tags, and in JSON format with \"action\" and \"action_input\".\n\nAfter calling the function, the response from the function is given as \"Function: 40 C\". This means the function call was successful, and now I have the weather information.\n\nMy response should be helpful and clear. Since the function call has been made and I have the result, I should now provide the information to the user.\n\nThe system prompt says: \"Keep your response to user clear; please do not make your response verbose!\" So, I should be concise.\n\nIn multi-turn conversations, I should continue calling functions if needed, but in this case, the function call has already been made in the first response, and now I have the result. The user's request is to know the weather, and I now have that information.\n\nLooking back at the interaction:\n\n- User's query: \"What's the weather in Florence, Italy?\"\n\n- My first response: I called the function.\n\n- Now, the function response is given: \"Function: 40 C\"\n\nI need to respond to the user with this information.\n\nThe system prompt indicates that after a function call, I should use the result to respond. Since the function call was in my previous response, and now I'm generating the next response, I should present the weather information clearly.\n\nFinal response structure: I should not include another function call unless more is needed. Here, the request is fulfilled with the weather data.\n\nSo, my response should be something like: \"The weather in Florence, Italy is 40°C.\"\n\nMake it clear and not verbose.","refusal":null,"role":"assistant","tool_calls":null},"stop_reason":null}],"created":1759350324,"id":"fd6c6475-ce69-4fc8-8b2d-42710d334cca_sidecar","model":"grok-3-mini-high","object":"chat.completion","prompt_filter_results":[{"prompt_index":0,"content_filter_results":{"hate":{"filtered":false,"severity":"safe"},"jailbreak":{"filtered":false,"detected":false},"self_harm":{"filtered":false,"severity":"safe"},"sexual":{"filtered":false,"severity":"safe"},"violence":{"filtered":false,"severity":"safe"}}}],"usage":{"audio_prompt_tokens":0,"completion_tokens":11,"completion_tokens_details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":417,"rejected_prediction_tokens":0},"num_sources_used":0,"prompt_tokens":332,"prompt_tokens_details":{"audio_tokens":0,"cached_tokens":0,"image_tokens":0,"text_tokens":332},"total_tokens":760}}
    headers:
      Content-Type:
      - application/json
    status: 200 OK
    code: 200
    duration: 5.354268666s
