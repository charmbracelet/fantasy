---
version: 2
interactions:
- id: 0
  request:
    proto: HTTP/1.1
    proto_major: 1
    proto_minor: 1
    content_length: 453
    host: ""
    body: '{"messages":[{"content":"You are a helpful assistant","role":"system"},{"content":"What''s the weather in Florence,Italy?","role":"user"}],"model":"grok-3-mini","max_tokens":4000,"tool_choice":"auto","tools":[{"function":{"name":"weather","strict":false,"description":"Get weather information for a location","parameters":{"properties":{"location":{"description":"the city","type":"string"}},"required":["location"],"type":"object"}},"type":"function"}]}'
    headers:
      Accept:
      - application/json
      Content-Type:
      - application/json
      User-Agent:
      - OpenAI/Go 2.7.1
    url: https://fantasy-playground-resource.openai.azure.com/openai/v1/chat/completions
    method: POST
  response:
    proto: HTTP/2.0
    proto_major: 2
    proto_minor: 0
    content_length: -1
    uncompressed: true
    body: |
      {"choices":[{"content_filter_results":{},"finish_reason":"tool_calls","index":0,"logprobs":null,"message":{"content":"","reasoning_content":"First, the user is asking about the weather in Florence, Italy. I have an available function called \"weather\" that can get weather information for a location.\n\nThe function description is: \"Get weather information for a location\". The parameters require a \"location\" which is a string describing the city.\n\nThe user provided \"Florence, Italy\", which clearly specifies the city and country. This matches the required parameter for the \"weather\" function.\n\nAll required parameters are provided and inferable. The location is \"Florence, Italy\", so I can use that directly.\n\nI should call the function if it can address the query. Here, it directly can, so I need to trigger a function call.\n\nThe function call format is specific: use \u003cfunction_call\u003e tags with JSON inside, like \u003cfunction_call\u003e{\"action\": \"function_name\", \"action_input\": {\"parameter\": \"argument\"}}\u003c/function_call\u003e.\n\nFor this, the action is \"weather\", and the action_input should have \"location\" with the value \"Florence, Italy\".\n\nI don't need to add any extra response text unless necessary. The instructions say to keep responses clear and not verbose, so I should just make the function call if that's the next step.\n\nThis is a single-turn query that can be handled by the function, so I'll call it directly.\n\nFinal decision: Call the \"weather\" function with the location \"Florence, Italy\".","refusal":null,"role":"assistant","tool_calls":[{"function":{"arguments":"{\"location\":\"Florence, Italy\"}","call_id":null,"name":"weather"},"id":"call_03413597","type":"function"}]},"stop_reason":null}],"created":1761842984,"id":"0a9e19c6-4d29-4a05-a60d-2923fdd75f2f_sidecar","model":"grok-3-mini-high","object":"chat.completion","prompt_filter_results":[{"prompt_index":0,"content_filter_results":{"hate":{"filtered":false,"severity":"safe"},"jailbreak":{"filtered":false,"detected":false},"self_harm":{"filtered":false,"severity":"safe"},"sexual":{"filtered":false,"severity":"safe"},"violence":{"filtered":false,"severity":"safe"}}}],"usage":{"audio_prompt_tokens":0,"completion_tokens":27,"completion_tokens_details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":281,"rejected_prediction_tokens":0},"num_sources_used":0,"prompt_tokens":288,"prompt_tokens_details":{"audio_tokens":0,"cached_tokens":0,"image_tokens":0,"text_tokens":288},"total_tokens":596}}
    headers:
      Content-Type:
      - application/json
    status: 200 OK
    code: 200
    duration: 2.080312709s
- id: 1
  request:
    proto: HTTP/1.1
    proto_major: 1
    proto_minor: 1
    content_length: 673
    host: ""
    body: '{"messages":[{"content":"You are a helpful assistant","role":"system"},{"content":"What''s the weather in Florence,Italy?","role":"user"},{"tool_calls":[{"id":"call_03413597","function":{"arguments":"{\"location\":\"Florence, Italy\"}","name":"weather"},"type":"function"}],"role":"assistant"},{"content":"40 C","tool_call_id":"call_03413597","role":"tool"}],"model":"grok-3-mini","max_tokens":4000,"tool_choice":"auto","tools":[{"function":{"name":"weather","strict":false,"description":"Get weather information for a location","parameters":{"properties":{"location":{"description":"the city","type":"string"}},"required":["location"],"type":"object"}},"type":"function"}]}'
    headers:
      Accept:
      - application/json
      Content-Type:
      - application/json
      User-Agent:
      - OpenAI/Go 2.7.1
    url: https://fantasy-playground-resource.openai.azure.com/openai/v1/chat/completions
    method: POST
  response:
    proto: HTTP/2.0
    proto_major: 2
    proto_minor: 0
    content_length: -1
    uncompressed: true
    body: |
      {"choices":[{"content_filter_results":{"hate":{"filtered":false,"severity":"safe"},"protected_material_code":{"filtered":false,"detected":false},"protected_material_text":{"filtered":false,"detected":false},"self_harm":{"filtered":false,"severity":"safe"},"sexual":{"filtered":false,"severity":"safe"},"violence":{"filtered":false,"severity":"safe"}},"finish_reason":"stop","index":0,"logprobs":null,"message":{"content":"The weather in Florence, Italy is currently 40°C.","reasoning_content":"First, the user asked: \"What's the weather in Florence, Italy?\" This directly matches the available function \"weather\", which gets weather information for a location.\n\nThe function requires a \"location\" parameter, which is a string. The user provided \"Florence, Italy\", so all required parameters are present and clear. No need to ask for clarification.\n\nI should call the function using the specified JSON format within \u003cfunction_call\u003e tags. The format is: \u003cfunction_call\u003e{\"action\": \"function_name\", \"action_input\": {\"parameter\": \"argument\"}}\u003c/function_call\u003e\n\nSo, for this, it should be: \u003cfunction_call\u003e{\"action\": \"weather\", \"action_input\": {\"location\": \"Florence, Italy\"}}\u003c/function_call\u003e\n\nAfter calling the function, the response from the function is given as \"Function: 40 C\". This seems to be the result of the function call.\n\nNow, in my response as the assistant, I need to handle this. The system prompt says: \"You are a helpful assistant\" and I should respond based on the function call. Since I've made the function call, and now I have the result, I should provide a clear, non-verbose response to the user with the information.\n\nThe conversation is multi-turn, but in this case, the function call has been made, and I have the result. I need to present this result to the user.\n\nMy response should not be verbose, so I'll keep it concise. Something like: \"The weather in Florence, Italy is 40°C.\"\n\nThe function response is \"40 C\", which I assume means 40 degrees Celsius. I should make it user-friendly.\n\nFinally, ensure that I'm not making another function call unless needed. Here, the request is fulfilled with this information, so no need for further calls.\n\nResponse structure:\n- Since the function call was already made in the first response, now I need to provide the answer based on the function's output.","refusal":null,"role":"assistant","tool_calls":null},"stop_reason":null}],"created":1761842986,"id":"1792caed-512b-4e35-970a-614f14ed1aba_sidecar","model":"grok-3-mini-high","object":"chat.completion","prompt_filter_results":[{"prompt_index":0,"content_filter_results":{"hate":{"filtered":false,"severity":"safe"},"jailbreak":{"filtered":false,"detected":false},"self_harm":{"filtered":false,"severity":"safe"},"sexual":{"filtered":false,"severity":"safe"},"violence":{"filtered":false,"severity":"safe"}}}],"usage":{"audio_prompt_tokens":0,"completion_tokens":12,"completion_tokens_details":{"accepted_prediction_tokens":0,"audio_tokens":0,"reasoning_tokens":394,"rejected_prediction_tokens":0},"num_sources_used":0,"prompt_tokens":332,"prompt_tokens_details":{"audio_tokens":0,"cached_tokens":0,"image_tokens":0,"text_tokens":332},"total_tokens":738}}
    headers:
      Content-Type:
      - application/json
    status: 200 OK
    code: 200
    duration: 2.818933708s
